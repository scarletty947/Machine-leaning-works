import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt

# 1. Data Preparation
# Define transformations for the training and test sets
transform = transforms.Compose([
    transforms.ToTensor(),  # Convert PIL image to PyTorch Tensor å°†åƒç´ å€¼ä» [0, 255] æ˜ å°„åˆ° [0.0, 1.0] çš„æµ®ç‚¹æ•°
    transforms.Normalize((0.1307,), (0.3081,))  # Normalize the dataset ğ‘¥norm=ğ‘¥âˆ’ğœ‡/ğœæ ‡å‡†åŒ–ä¸º0å‡å€¼ å•ä½æ–¹å·®
    ])

# Download and load the training and test datasets
train_dataset = datasets.MNIST(root='./data', train=True,
                               transform=transform, download=True)
test_dataset = datasets.MNIST(root='./data', train=False,
                              transform=transform, download=True)

# Create data loaders
# train_loader æ¯è°ƒç”¨ä¸€æ¬¡è¿”å›ä¸€ä¸ªæ–°çš„è¿­ä»£å¯¹è±¡  è¿­ä»£éšæœºåˆ†æˆçš„å¤šä¸ªbatch
train_loader = DataLoader(dataset=train_dataset, batch_size=32, shuffle=True)
test_loader = DataLoader(dataset=test_dataset, batch_size=32, shuffle=False)

# 2. Model Construction
class MLP(nn.Module):
    def __init__(self):
        super(MLP, self).__init__()
        self.flatten = nn.Flatten()
        self.hidden = nn.Linear(28*28, 32)  # Input layer to hidden layer
        self.relu = nn.ReLU()
        self.output = nn.Linear(32, 10)     # Hidden layer to output layer
        self.softmax = nn.LogSoftmax(dim=1)  # Use LogSoftmax for numerical stability

    def forward(self, x):
        x = self.flatten(x)
        x = self.hidden(x)
        x = self.relu(x)
        x = self.output(x)
        x = self.softmax(x)
        return x

model = MLP()
#the number of parameters in the shallow network is 32*784+32+10*32+10=25,450
#the number of parameters in the deep network is 30*784+30+28*30+28+26*28+26+10*26+10=25442
#deep network
class DMLP(nn.Module):
    def __init__(self):
        super(DMLP, self).__init__()
        self.flatten = nn.Flatten()
        self.l1 = nn.Linear(784, 30)
        self.l2 = nn.Linear(30, 28)
        self.l3 = nn.Linear(28, 26)
        self.output = nn.Linear(26, 10)     # Hidden layer to output layer
        self.relu = nn.ReLU()
        self.softmax = nn.LogSoftmax(dim=1)  # Use LogSoftmax for numerical stability

    def forward(self, x):
        x = self.flatten(x)
        x = self.relu(self.l1(x))
        x = self.relu(self.l2(x))
        x = self.relu(self.l3(x))
        x = self.output(x)
        x = self.softmax(x)
        return x

deep_model = DMLP()
# 3. Model Compilation
#å®ƒå†…éƒ¨ä¼šè‡ªåŠ¨å¯¹ output åš log_softmax,ç„¶åç”¨ target çš„æ•´æ•°ç´¢å¼•æ¥é€‰å–å¯¹åº”ç±»åˆ«çš„æ¦‚ç‡
criterion = nn.NLLLoss()  # Negative Log Likelihood Loss (used with LogSoftmax)
optimizer = optim.SGD(model.parameters(), lr=0.01)# lr learning rate
deep_optimizer = optim.SGD(deep_model.parameters(), lr=0.01)# lr learning rate

# 4. Model Training
epochs = 20
train_losses = []
valid_losses = []
deep_train_losses = []
train_accuracy = []
deep_train_accuracy = []

for epoch in range(epochs):
    model.train()
    epoch_loss = 0
    correct = 0

    for data, target in train_loader:
        optimizer.zero_grad()# æ¸…é™¤æ—§æ¢¯åº¦ ä¸Šä¸€ä¸ªbatch
        output = model(data)
        loss = criterion(output, target)  # target is not one-hot encoded in PyTorch one-hotç”¨ä¸€ä¸ªé•¿åº¦ä¸ºç±»åˆ«æ€»æ•°çš„å‘é‡è¡¨ç¤ºæ¯ä¸ªç±»åˆ«ï¼Œå…¶ä¸­åªæœ‰ä¸€ä¸ªä½ç½®æ˜¯ 1ï¼Œå…¶ä½™éƒ½æ˜¯ 0ã€‚ output: logits
        loss.backward()# åå‘ä¼ æ’­ï¼Œè®¡ç®—æŸå¤±å¯¹å‚æ•°çš„æ¢¯åº¦
        optimizer.step()# æ›´æ–°å‚æ•°

        epoch_loss += loss.item()
        #æ²¿ç€ç¬¬ 1 ç»´ï¼ˆå³æ¯è¡Œï¼‰æ‰¾å‡ºæœ€å¤§å€¼çš„ç´¢å¼•,ä¹Ÿå°±æ˜¯æ¯ä¸ªæ ·æœ¬é¢„æµ‹å¾—åˆ†æœ€é«˜çš„ç±»åˆ«ç´¢å¼• keepdimä¿ç•™åŸæ¥çš„ç»´åº¦ç»“æ„
        pred = output.argmax(dim=1, keepdim=True)
        correct += pred.eq(target.view_as(pred)).sum().item()#å½“å‰è®­ç»ƒçš„æ­£ç¡®æ ·æœ¬æ•°

    train_losses.append(epoch_loss / len(train_loader))#å¹³å‡è®­ç»ƒæŸå¤±
    train_accuracy.append(100. * correct / len(train_loader.dataset)) 

    print(f'Epoch {epoch+1}/{epochs}, Loss: {train_losses[-1]:.4f}, Accuracy: {train_accuracy[-1]:.2f}%')


for epoch in range(epochs):
    deep_model.train()
    epoch_loss = 0
    correct = 0

    for data, target in train_loader:
        deep_optimizer.zero_grad()# æ¸…é™¤æ—§æ¢¯åº¦ ä¸Šä¸€ä¸ªbatch
        output = deep_model(data)
        loss = criterion(output, target)  # target is not one-hot encoded in PyTorch one-hotç”¨ä¸€ä¸ªé•¿åº¦ä¸ºç±»åˆ«æ€»æ•°çš„å‘é‡è¡¨ç¤ºæ¯ä¸ªç±»åˆ«ï¼Œå…¶ä¸­åªæœ‰ä¸€ä¸ªä½ç½®æ˜¯ 1ï¼Œå…¶ä½™éƒ½æ˜¯ 0ã€‚ output: logits
        loss.backward()# åå‘ä¼ æ’­ï¼Œè®¡ç®—æŸå¤±å¯¹å‚æ•°çš„æ¢¯åº¦
        deep_optimizer.step()# æ›´æ–°å‚æ•°

        epoch_loss += loss.item()
        #æ²¿ç€ç¬¬ 1 ç»´ï¼ˆå³æ¯è¡Œï¼‰æ‰¾å‡ºæœ€å¤§å€¼çš„ç´¢å¼•,ä¹Ÿå°±æ˜¯æ¯ä¸ªæ ·æœ¬é¢„æµ‹å¾—åˆ†æœ€é«˜çš„ç±»åˆ«ç´¢å¼• keepdimä¿ç•™åŸæ¥çš„ç»´åº¦ç»“æ„
        pred = output.argmax(dim=1, keepdim=True)
        correct += pred.eq(target.view_as(pred)).sum().item()#å½“å‰è®­ç»ƒçš„æ­£ç¡®æ ·æœ¬æ•°

    deep_train_losses.append(epoch_loss / len(train_loader))#å¹³å‡è®­ç»ƒæŸå¤±
    deep_train_accuracy.append(100. * correct / len(train_loader.dataset))

    print(f'Epoch {epoch+1}/{epochs}, Loss: {deep_train_losses[-1]:.4f}, Accuracy: {deep_train_accuracy[-1]:.2f}%')

#compare the train process between shallow and deep network
# åˆ›å»ºå›¾å½¢
plt.figure(figsize=(10, 6))
plt.plot(list(range(1, epochs+1)), train_losses, label='Shallow MLP', marker='o')
plt.plot(list(range(1, epochs+1)), deep_train_losses, label='Deep MLP', marker='s')
plt.xticks(range(1, epochs+1))  # å¼ºåˆ¶ x è½´æ˜¾ç¤ºæ•´æ•°åˆ»åº¦
# æ·»åŠ æ ‡é¢˜å’Œæ ‡ç­¾
plt.title('Loss vs Epoch for Two Models')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.tight_layout()

# æ˜¾ç¤ºå›¾å½¢
plt.show()
# 5. Model Evaluation
model.eval()
test_loss = 0
correct = 0
#with æ˜¯ Python ä¸­çš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼ˆcontext managerï¼‰è¯­æ³•ï¼Œå®ƒçš„ä½œç”¨æ˜¯ï¼š
#åœ¨ä¸€æ®µä»£ç å—æ‰§è¡Œå‰åè‡ªåŠ¨å¤„ç†èµ„æºçš„åˆå§‹åŒ–å’Œæ¸…ç†å·¥ä½œã€‚
#åœ¨è¿™ä¸ªä»£ç å—ä¸­ï¼Œä¸è¦è¿½è¸ªæ¢¯åº¦è®¡ç®—ï¼ˆno gradient trackingï¼‰
with torch.no_grad():
    for data, target in test_loader:
        output = model(data)
        test_loss += criterion(output, target).item()
        pred = output.argmax(dim=1, keepdim=True)
        correct += pred.eq(target.view_as(pred)).sum().item()

test_loss /= len(test_loader)
test_accuracy = 100. * correct / len(test_loader.dataset)

print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')





